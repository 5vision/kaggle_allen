{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lucene\n",
    "import os\n",
    "import book_processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from lucene import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f6d042ba210>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"data/training_set.tsv\", sep = '\\t')\n",
    "data_val = pd.read_csv(\"data/validation_set.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDocument(fname):\n",
    "    doc = Document()\n",
    "    doc.add(Field('filename', os.path.split(fname)[-1], Field.Store.YES, Field.Index.NOT_ANALYZED))\n",
    "    doc.add(Field('content', open(fname).read(), Field.Store.YES, Field.Index.ANALYZED))\n",
    "    return doc\n",
    "\n",
    "def indexFile(fname, writer):\n",
    "    doc = getDocument(fname)\n",
    "    writer.addDocument(doc)\n",
    "\n",
    "def indexDirectory(dir_path, writer):\n",
    "    for fname in os.listdir(dir_path):\n",
    "        indexFile(os.path.join(dir_path, fname), writer)\n",
    "    return writer.numDocs()\n",
    "\n",
    "def indexDictionary(d, writer):\n",
    "    for k, v in d.iteritems():\n",
    "        doc = Document()\n",
    "        doc.add(Field('filename', k, Field.Store.YES, Field.Index.NOT_ANALYZED))\n",
    "        doc.add(Field('content', v, Field.Store.YES, Field.Index.ANALYZED))\n",
    "        writer.addDocument(doc)\n",
    "    return writer.numDocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2139"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index wiki articles based on ck 12 topics\n",
    "#analyzer = StandardAnalyzer(Version.LUCENE_30)\n",
    "analyzer = SnowballAnalyzer(Version.LUCENE_30, \"English\", StandardAnalyzer.STOP_WORDS_SET)\n",
    "writer = IndexWriter(SimpleFSDirectory(File(\"data/index/wiki_ck12\")), analyzer, True, IndexWriter.MaxFieldLength.UNLIMITED)\n",
    "indexDirectory('data/wiki_data', writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1723"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index topics from ck12 book (document is text between h1 tags)\n",
    "dir_name = 'data/ck12_book/OEBPS'\n",
    "docs = {}\n",
    "html_paths = [os.path.join(dir_name,  str(i+1) + '.html') for i in range(124)]\n",
    "for f_name in html_paths:\n",
    "    docs.update(book_processing.get_h1_text(open(f_name).read()))\n",
    "\n",
    "#analyzer = StandardAnalyzer(Version.LUCENE_30)\n",
    "analyzer = SnowballAnalyzer(Version.LUCENE_30, \"English\", StandardAnalyzer.STOP_WORDS_SET)\n",
    "writer = IndexWriter(SimpleFSDirectory(File(\"data/index/ck12_books_topics\")), analyzer, True, IndexWriter.MaxFieldLength.UNLIMITED)\n",
    "indexDictionary(docs, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10040"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index paragraphs from ck12 book (document is text between any h tags)\n",
    "dir_name = 'data/ck12_book/OEBPS'\n",
    "docs = {}\n",
    "html_paths = [os.path.join(dir_name,  str(i+1) + '.html') for i in range(124)]\n",
    "for f_name in html_paths:\n",
    "    docs.update(book_processing.get_h_all_text(open(f_name).read()))\n",
    "\n",
    "#analyzer = StandardAnalyzer(Version.LUCENE_30)\n",
    "analyzer = SnowballAnalyzer(Version.LUCENE_30, \"English\", StandardAnalyzer.STOP_WORDS_SET)\n",
    "writer = IndexWriter(SimpleFSDirectory(File(\"data/index/ck12_books_paragraphs\")), analyzer, True, IndexWriter.MaxFieldLength.UNLIMITED)\n",
    "indexDictionary(docs, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = {}\n",
    "MAX = 100\n",
    "docs_per_q = range(1,20)\n",
    "\n",
    "#analyzer = StandardAnalyzer(Version.LUCENE_30)\n",
    "analyzer = SnowballAnalyzer(Version.LUCENE_30, \"English\", StandardAnalyzer.STOP_WORDS_SET)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(\"data/index/ck12_books_paragraphs\")))\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "for index, row in data_test.iterrows():\n",
    "        \n",
    "    queries = [row['answerA'], row['answerB'], row['answerC'], row['answerD']]\n",
    "    queries = [row['question'] + ' ' + q  for q in queries]\n",
    "        \n",
    "    scores = {}\n",
    "    for q in queries:\n",
    "        query = QueryParser(Version.LUCENE_30, \"content\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", q))\n",
    "        #query = QueryParser(Version.LUCENE_30, \"content\", analyzer).parse(re.sub(\"[/^]\", \"\\^\", q))\n",
    "        hits = searcher.search(query, MAX)\n",
    "        doc_importance = [hit.score for hit in hits.scoreDocs]\n",
    "        for n in docs_per_q:\n",
    "            scores.setdefault(n, [])\n",
    "            scores[n].append(sum(doc_importance[:n]))\n",
    "      \n",
    "    for n in docs_per_q:\n",
    "        res.setdefault(n, [])\n",
    "        res[n].append(['A','B','C','D'][np.argmax(scores[n])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.42\n",
      "2 0.4308\n",
      "3 0.4412\n",
      "4 0.444\n",
      "5 0.4404\n",
      "6 0.44\n",
      "7 0.4368\n",
      "8 0.4372\n",
      "9 0.4396\n",
      "10 0.4408\n",
      "11 0.4376\n",
      "12 0.4364\n",
      "13 0.4384\n",
      "14 0.4356\n",
      "15 0.4348\n",
      "16 0.4336\n",
      "17 0.4332\n",
      "18 0.4296\n",
      "19 0.4276\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(res.items(), key=lambda x: x[0]):\n",
    "    print k, 1. * sum(data_test['correctAnswer'] == v) / len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#alternative prediction, first get top scored questions and then calculate scores for answers\n",
    "res = {}\n",
    "MAX = 30\n",
    "docs_per_q = range(1,20)\n",
    "\n",
    "#analyzer = StandardAnalyzer(Version.LUCENE_30)\n",
    "analyzer = SnowballAnalyzer(Version.LUCENE_30, \"English\", StandardAnalyzer.STOP_WORDS_SET)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(\"data/index/ck12_books_paragraphs\")))\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "for index, row in data_test.iterrows():\n",
    "    q =  row['question']\n",
    "    query = QueryParser(Version.LUCENE_30, \"content\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", q))\n",
    "    hits = searcher.search(query, MAX)\n",
    "    \n",
    "    sc_A = []\n",
    "    sc_B = []\n",
    "    sc_C = []\n",
    "    sc_D = []\n",
    "    \n",
    "    qA = QueryParser(Version.LUCENE_30, \"content\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", row['answerA']))\n",
    "    qB = QueryParser(Version.LUCENE_30, \"content\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", row['answerB']))\n",
    "    qC = QueryParser(Version.LUCENE_30, \"content\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", row['answerC']))\n",
    "    qD = QueryParser(Version.LUCENE_30, \"content\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", row['answerD']))\n",
    "    \n",
    "    \n",
    "    for hit in hits.scoreDocs:\n",
    "        sc_A.append(searcher.explain(qA, hit.doc).getValue())\n",
    "        sc_B.append(searcher.explain(qB, hit.doc).getValue())\n",
    "        sc_C.append(searcher.explain(qC, hit.doc).getValue())\n",
    "        sc_D.append(searcher.explain(qD, hit.doc).getValue())\n",
    "    for n in docs_per_q:\n",
    "        res.setdefault(n, [])\n",
    "        res[n].append(['A','B','C','D'][np.argmax([sum(sc_A[:n]), sum(sc_B[:n]), sum(sc_C[:n]), sum(sc_D[:n])])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.3516\n",
      "2 0.3924\n",
      "3 0.4028\n",
      "4 0.412\n",
      "5 0.4208\n",
      "6 0.42\n",
      "7 0.4248\n",
      "8 0.4312\n",
      "9 0.4292\n",
      "10 0.4296\n",
      "11 0.428\n",
      "12 0.4288\n",
      "13 0.4288\n",
      "14 0.4296\n",
      "15 0.4308\n",
      "16 0.4328\n",
      "17 0.4352\n",
      "18 0.436\n",
      "19 0.4348\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(res.items(), key=lambda x: x[0]):\n",
    "    print k, 1. * sum(data_test['correctAnswer'] == v) / len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save result\n",
    "pd.DataFrame({'id': list(data_val['id']), 'correctAnswer': res[8]})[['id', 'correctAnswer']].to_csv(\"prediction_ck12_h1_8_lucene_snowball_q_first.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
